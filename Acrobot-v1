import tensorflow as tf
import numpy as np
import gym
import random
from collections import  deque

EPISODE = 5000
STEP = 500
TEST = 5
hl_n = 128
LR = 0.001
INIT_EPSILON = 0.9
FINAL_EPSILON = 0.01
GAMMA = 0.99
REPLACE_TARGET = 5

MAX_MEMORY = 500
BATCH_SIZE = 32

class DDQN():
    def __init__(self, env):
        self.s_dim = env.observation_space.shape[0]
        self.a_dim = env.action_space.n

        self.memory = deque()
        self.epsilon = INIT_EPSILON

        self.sess = tf.Session()

        self.s_input = tf.placeholder(tf.float32,[None,self.s_dim])
        self.a_input = tf.placeholder(tf.float32,[None,self.a_dim])

        with tf.variable_scope('eval'):
            h_layer = tf.layers.dense(self.s_input,hl_n,activation=tf.nn.sigmoid,trainable=True)
            self.q = tf.layers.dense(h_layer,self.a_dim,trainable=True)

        with tf.variable_scope('target'):
            h_layer = tf.layers.dense(self.s_input,hl_n,activation=tf.nn.sigmoid,trainable=False)
            self.q_ = tf.layers.dense(h_layer,self.a_dim,trainable=False)

        self.eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope = 'eval')
        self.target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope = 'target')

        self.update_params = [tf.assign(t,e) for t,e in zip(self.target_params,self.eval_params)]

        self.y_input = tf.placeholder(tf.float32,[None,1])

        q_action = tf.reduce_sum(tf.multiply(self.q,self.a_input),reduction_indices=1)
        self.loss = tf.reduce_mean(tf.square(q_action - self.y_input))
        self.optimizer = tf.train.AdamOptimizer(LR).minimize(self.loss)

        self.sess.run(tf.global_variables_initializer())

    def update_target_params(self):
        self.sess.run(self.update_params)

    def store(self,state,action,reward,next_state,done):
        action_in = np.zeros(self.a_dim)
        action_in[action+1] = 1

        self.memory.append((state,action_in,[reward],next_state,done))

        if len(self.memory) > MAX_MEMORY:
            self.memory.popleft()

        if len(self.memory) > BATCH_SIZE:
            self.train()

    def train(self):
        minibatch = random.sample(self.memory,BATCH_SIZE)

        s_batch = [data[0] for data in minibatch]
        a_batch = [data[1] for data in minibatch]
        r_batch = [data[2] for data in minibatch]
        st_batch = [data[3] for data in minibatch]
        done_batch = [data[4] for data in minibatch]

        y_th = []

        q_value = self.sess.run(self.q,{self.s_input:np.vstack(st_batch)})
        max_action = np.argmax(q_value,axis=1)
        qt_value = self.sess.run(self.q_,{self.s_input: np.vstack(st_batch)})

        for i in range(0,BATCH_SIZE):
            if done_batch[i]:
                y_th.append(r_batch[i])
            else:
                y_th.append(r_batch[i]+GAMMA * qt_value[i,max_action[i]])

        y_in = np.vstack(y_th)
        self.sess.run(self.optimizer,{self.s_input: np.vstack(s_batch),self.a_input:np.vstack(a_batch),self.y_input:y_in})

    def egreedy(self,state):
        q_value = self.sess.run(self.q,{self.s_input:state[np.newaxis, :]})[0]

        if random.random() <= self.epsilon:
            self.epsilon -= (INIT_EPSILON-FINAL_EPSILON)/10000
            return random.randint(-1,1)

        else:
            self.epsilon -= (INIT_EPSILON - FINAL_EPSILON)/10000
            return np.argmax(q_value)-1

    def choose_action(self,state,episode):
        q_value = self.sess.run(self.q, {self.s_input: state[np.newaxis, :]})[0]

        if episode >= 600:
            a = q_value

        return np.argmax(q_value)-1



if __name__ == '__main__':
    env = gym.make('Acrobot-v1')
    env = env.unwrapped
    env.seed(1)

    agent = DDQN(env)

    state = env.reset()

    for episode in range(EPISODE):
        s = env.reset()
        for step in range(STEP):
            a = agent.egreedy(s)

            s_, r, done, _ = env.step(a)

            if done and step < STEP-1:
                r = 100
            else:
                r = 1-s[0]

            agent.store(s,a,r,s_,done)
            s = s_

            if done:
                break

        if episode%100 == 0:
            tot_reward = 0
            for test in range(TEST):
                s = env.reset()
                for step in range(STEP):
                    env.render()
                    a = agent.choose_action(s,episode)
                    s,r,done,_ = env.step(a)
                    tot_reward += r

                    if done:
                        break
            print(episode,"::::",tot_reward/TEST)

        if episode%REPLACE_TARGET == 0:
            agent.update_target_params()
