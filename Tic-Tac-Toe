import numpy as np
import pickle

BOARD_ROWS = 3
BOARD_COLS = 3
BOARD_SIZE = BOARD_ROWS * BOARD_COLS

class State:
    def __init__(self):
        # the board is represented by an n * n array,
        # 1 represents a chessman of the player who moves first,
        # -1 represents a chessman of another player
        # 0 represents an empty position
        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.winner = None
        self.hash_val = None
        self.end = None

    # compute the hash value for one state, it's unique
    def hash(self):#获取该状态的hash值(等于抽象为一个值)，主要是为了保存方便
        if self.hash_val is None:
            self.hash_val = 0
            for i in self.data.reshape(BOARD_ROWS * BOARD_COLS):
                if i == -1:
                    i = 2
                self.hash_val = self.hash_val * 3 + i
        return int(self.hash_val)

    # check whether a player has won the game, or it's a tie
    def is_end(self):#判断当前局面是否已经有了输赢
        if self.end is not None:
            return self.end
        results = []
        # check row
        for i in range(0, BOARD_ROWS):
            results.append(np.sum(self.data[i, :]))
        # check columns
        for i in range(0, BOARD_COLS):
            results.append(np.sum(self.data[:, i]))

        # check diagonals
        results.append(0)
        for i in range(0, BOARD_ROWS):
            results[-1] += self.data[i, i]
        results.append(0)
        for i in range(0, BOARD_ROWS):
            results[-1] += self.data[i, BOARD_ROWS - 1 - i]

        for result in results:
            if result == 3:
                self.winner = 1
                self.end = True
                return self.end
            if result == -3:
                self.winner = -1
                self.end = True
                return self.end

        # whether it's a tie
        sum = np.sum(np.abs(self.data))
        if sum == BOARD_ROWS * BOARD_COLS:
            self.winner = 0
            self.end = True
            return self.end

        # game is still going on
        self.end = False
        return self.end

    # @symbol: 1 or -1
    # put chessman symbol in position (i, j)
    def next_state(self, i, j, symbol):#当前状态下在(i,j)位置下了symbol这个子之后得到的新状态
        new_state = State()
        new_state.data = np.copy(self.data)
        new_state.data[i, j] = symbol
        return new_state

    # print the board
    def print(self):#可视化棋盘
        for i in range(0, BOARD_ROWS):
            print('-------------')
            out = '| '
            for j in range(0, BOARD_COLS):
                if self.data[i, j] == 1:
                    token = '*'
                if self.data[i, j] == 0:
                    token = '0'
                if self.data[i, j] == -1:
                    token = 'x'
                out += token + ' | '
            print(out)
        print('-------------')

def get_all_states_impl(current_state, current_symbol, all_states): #暴力循环所有棋盘状态，并加入all_states状态库
    for i in range(0, BOARD_ROWS):
        for j in range(0, BOARD_COLS):
            if current_state.data[i][j] == 0:
                newState = current_state.next_state(i, j, current_symbol)
                newHash = newState.hash()
                if newHash not in all_states.keys():
                    isEnd = newState.is_end()
                    all_states[newHash] = (newState, isEnd)
                    if not isEnd:
                        get_all_states_impl(newState, -current_symbol, all_states)

def get_all_states():
    current_symbol = 1
    current_state = State()
    all_states = dict()
    all_states[current_state.hash()] = (current_state, current_state.is_end())
    get_all_states_impl(current_state, current_symbol, all_states)
    return all_states

# all possible board configurations
all_states = get_all_states()

class Judger:
    # @player1: the player who will move first, its chessman will be 1
    # @player2: another player with a chessman -1
    # @feedback: if True, both players will receive rewards when game is end
    def __init__(self, player1, player2):
        self.p1 = player1
        self.p2 = player2
        self.current_player = None
        self.p1_symbol = 1
        self.p2_symbol = -1
        self.p1.set_symbol(self.p1_symbol)
        self.p2.set_symbol(self.p2_symbol)
        self.current_state = State()

    def reset(self):
        self.p1.reset()
        self.p2.reset()

    def alternate(self):
        while True:
            yield self.p1
            yield self.p2

    # @print: if True, print each board during the game
    def play(self, print=False):
        alternator = self.alternate()#用于在两个玩家间循环
        self.reset()#初始化两个玩家的各种状态
        current_state = State()#初始化棋盘
        self.p1.set_state(current_state)#玩家1初始化棋盘及greedy
        self.p2.set_state(current_state)#玩家2初始化棋盘及greedy
        while True:
            player = next(alternator)#轮流下棋
            if print:
                current_state.print()
            [i, j, symbol] = player.act()#玩家下棋操作，返回为当前最佳（或因触发探索而随机）的下棋位置和玩家号
            next_state_hash = current_state.next_state(i, j, symbol).hash()#将棋子落下并获取棋盘状态hash
            current_state, is_end = all_states[next_state_hash]#从棋盘库里获取当前棋盘的状态
            self.p1.set_state(current_state)#将棋盘状态推入玩家1队列，即队列中保存了一路走来的每一步状态
            self.p2.set_state(current_state)#将棋盘状态推入玩家2队列，即队列中保存了一路走来的每一步状态
            if is_end:#如果当前已分出胜负，则：
                if print:#如果需要则打印
                    current_state.print()
                return current_state.winner#返回当前的赢家
            #否则继续下棋直到分出胜负

# AI player
class Player:
    # @step_size: the step size to update estimations
    # @epsilon: the probability to explore
    def __init__(self, step_size=0.1, epsilon=0.1):
        self.estimations = dict()
        self.step_size = step_size
        self.epsilon = epsilon
        self.states = []
        self.greedy = []

    def reset(self):
        self.states = []
        self.greedy = []

    def set_state(self, state):
        self.states.append(state)
        self.greedy.append(True)

    def set_symbol(self, symbol):#初始化时将输棋步的价值标为0，赢棋标为1，和棋标为0.5
        self.symbol = symbol
        for hash_val in all_states.keys():
            (state, is_end) = all_states[hash_val]
            if is_end:
                if state.winner == self.symbol:
                    self.estimations[hash_val] = 1.0
                elif state.winner == 0:
                    # we need to distinguish between a tie and a lose
                    self.estimations[hash_val] = 0.5
                else:
                    self.estimations[hash_val] = 0
            else:
                self.estimations[hash_val] = 0.5

    # update value estimation
    def backup(self):
        # for debug
        # print('player trajectory')
        # for state in self.states:
        #     state.print()

        self.states = [state.hash() for state in self.states]#改为每个棋盘状态的hash值

        for i in reversed(range(len(self.states) - 1)):#将棋盘状态hash反转遍历
            state = self.states[i]#获取hash值
            td_error = self.greedy[i] * (self.estimations[self.states[i + 1]] - self.estimations[state])#后步价值减前步价值即为奖励，如果最后一步输了，则这一步的奖励为负
            self.estimations[state] += self.step_size * td_error#奖励加权叠加到价值,（之所以从倒数第二步开始，是因为最后一步的价值在初始化时就定好了），如果奖励为负，则降低这步的价值，进一步导致前步价值受到影响

    # choose an action based on the state
    def act(self):#对于某个环境执行某个动作
        state = self.states[-1]
        next_states = []
        next_positions = []
        for i in range(BOARD_ROWS):
            for j in range(BOARD_COLS):
                if state.data[i, j] == 0:#遍历棋盘每个位置，如果该位置没有棋子则：
                    next_positions.append([i, j])#可能的落子位置添加进队列
                    next_states.append(state.next_state(i, j, self.symbol).hash())#对应下棋位置落子后的棋盘状态添加进队列

        if np.random.rand() < self.epsilon:#如果触发了探索
            action = next_positions[np.random.randint(len(next_positions))]#在所有可能落子的位置随机落子
            action.append(self.symbol)#推入对应的玩家号
            self.greedy[-1] = False#由于这一步不是正常学习得来，大概率效果不好可能会在后续计算的时候影响正常的学习路线，所以标记为不参与计算
            return action#将动作返回

        values = []#如果没有触发探索
        for hash, pos in zip(next_states, next_positions):#遍历每个落子后状态和落子位置
            values.append((self.estimations[hash], pos))#将落子位置和对应的奖励推入价值队列
        np.random.shuffle(values)#随机打乱value中的数据？
        values.sort(key=lambda x: x[0], reverse=True)#排降序
        action = values[0][1]
        action.append(self.symbol)#当前选择的最优操作及玩家号推入action
        return action#返回动作

    def save_policy(self):
        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:
            pickle.dump(self.estimations, f)

    def load_policy(self):
        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:
            self.estimations = pickle.load(f)

# human interface
# input a number to put a chessman
# | q | w | e |
# | a | s | d |
# | z | x | c |
class HumanPlayer:
    def __init__(self, **kwargs):
        self.symbol = None
        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']
        self.state = None
        return

    def reset(self):
        return

    def set_state(self, state):
        self.state = state

    def set_symbol(self, symbol):
        self.symbol = symbol
        return

    def backup(self, _):
        return

    def act(self):
        self.state.print()
        key = input("Input your position:")
        data = self.keys.index(key)
        i = data // int(BOARD_COLS)
        j = data % BOARD_COLS
        return (i, j, self.symbol)

def train(epochs):
    player1 = Player(epsilon=0.01)#建立玩家1，探索概率0.01
    player2 = Player(epsilon=0.01)#建立玩家2，探索概率0.01
    judger = Judger(player1, player2)#玩家1、2进入操作
    player1_win = 0.0#统计赢的次数
    player2_win = 0.0
    for i in range(1, epochs + 1):#循环训练次数
        winner = judger.play(print=False)#每次训练内容为下一盘棋，直到一方取得胜利并返回
        if winner == 1:
            player1_win += 1
        if winner == -1:
            player2_win += 1#为对应的玩家增加胜利盘数
        print('Epoch %d, player 1 win %.02f, player 2 win %.02f' % (i, player1_win / i, player2_win / i))#输出目前为止的训练成果
        player1.backup()#更新当前局每一步对于玩家1的价值
        player2.backup()#更新当前局每一步对于玩家2的价值
        judger.reset()#重置玩家的下棋状态，但是不会重置玩家每一步的价值记录（学习记录）
    player1.save_policy()#训练完的学习记录保存为文件
    player2.save_policy()#训练完的学习记录保存为文件

def compete(turns):
    player1 = Player(epsilon=0)
    player2 = Player(epsilon=0)
    judger = Judger(player1, player2)
    player1.load_policy()
    player2.load_policy()
    player1_win = 0.0
    player2_win = 0.0
    for i in range(0, turns):
        winner = judger.play()
        if winner == 1:
            player1_win += 1
        if winner == -1:
            player2_win += 1
        judger.reset()
    print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))

# The game is a zero sum game. If both players are playing with an optimal strategy, every game will end in a tie.
# So we test whether the AI can guarantee at least a tie if it goes second.
def play():
    while True:
        player1 = HumanPlayer()
        player2 = Player(epsilon=0)
        judger = Judger(player1, player2)
        player2.load_policy()
        winner = judger.play()
        if winner == player2.symbol:
            print("You lose!")
        elif winner == player1.symbol:
            print("You win!")
        else:
            print("It is a tie!")

if __name__ == '__main__':
    train(int(1e5))#训练
    compete(int(1e3))
    play()

# See PyCharm help at https://www.jetbrains.com/help/pycharm/


# See PyCharm help at https://www.jetbrains.com/help/pycharm/
